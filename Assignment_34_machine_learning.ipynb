{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0577f53-3b65-41d0-9daa-1e938c36f126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?\n",
    "\"\"\"\n",
    "Overfitting :- Overfitting occurs when a model becomes too complex and starts to memorize the noise and random fluctuations in the training data instead of learning the general underlying patterns.\n",
    "Underfitting :- Underfitting occurs when a model is too simple or lacks the capacity to capture the underlying patterns in the training data.\n",
    "\n",
    "consequences :- \n",
    "overfitting :- Low Bias, High Variance \n",
    "Underfitting :- High Bias, High Variance \n",
    "\n",
    "mitigation :- \n",
    "Overfitting can be mitigated by techniques such as regularization, cross-validation, early stopping, and increasing the size and diversity of the training data.\n",
    "Underfitting can be mitigated by increasing the model's complexity, collecting more training data, or adjusting the hyperparameters to allow the model to learn more effectively.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fab2c4-81f9-4964-9515-b47611c7e42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: How can we reduce overfitting? Explain in brief.\n",
    "\"\"\"\n",
    "Regularization: By adding a penalty term to the loss function, regularization techniques such as L1 or L2 regularization can prevent the model from becoming too complex and reduce overfitting.\n",
    "Cross-validation: It helps in estimating the model's performance on unseen data and allows for hyperparameter tuning to prevent overfitting.\n",
    "Early stopping: By monitoring the model's performance on a validation set during training, the training can be stopped when the performance starts to degrade, preventing overfitting.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd26d7b0-fd6d-44ac-a415-166e3552ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "\"\"\"\n",
    "Insufficient model complexity: Using a linear model to represent a non-linear relationship in the data.\n",
    "Limited training data: When the available training data is insufficient or lacks diversity to capture the underlying patterns.\n",
    "High regularization: Applying excessive regularization, which penalizes the model's complexity too much and results in underfitting.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d3519c-5d32-463e-a381-4b6ed6987a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "# variance, and how do they affect model performance?\n",
    "\"\"\"  The bias-variance tradeoff is a fundamental concept in machine learning. Bias refers to the error introduced by approximating a real-world problem with a simplified model, while variance refers to the model's sensitivity to fluctuations in the training data. The relationship between bias and variance can be described as follows:\n",
    "\n",
    "High bias models have low complexity and tend to oversimplify the underlying patterns in the data, resulting in underfitting and poor performance.\n",
    "High variance models have high complexity and are overly sensitive to the noise and fluctuations in the training data, leading to overfitting and poor generalization to new data.\n",
    "Balancing bias and variance is crucial for achieving good model performance. The goal is to find an optimal level of complexity that minimizes both bias and variance.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59950c40-94c6-4069-a81e-180609458b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "# How can you determine whether your model is overfitting or underfitting?\n",
    "\"\"\"\n",
    "Common methods for detecting overfitting and underfitting include:\n",
    "\n",
    "Train/Test split: Evaluating the model's performance on a separate test set that was not used during training can reveal overfitting or underfitting.\n",
    "Cross-validation: By dividing the data into multiple folds and evaluating the model's performance on different folds, cross-validation helps in assessing the model's ability to generalize.\n",
    "Learning curves: Plotting the model's performance (e.g., accuracy or loss) on the training and validation data as a function of the training set size can provide insights into overfitting or underfitting.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116364c6-114d-4dbb-8c93-0054573cb868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "# some common regularization techniques and how they work.\n",
    "\"\"\"\n",
    "Regularization in machine learning is a technique used to prevent overfitting by adding a penalty or constraint to the model's objective function.\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
